# Papers Read + Notes

 - ### Kwasnicka_Paradowski_Efficiency_Aspects_of_Neural_Network_Architecture_Evolution_Using_Direct_and_Indirect_Encoding
   - A suboptimally written short discussing direct vs indirect encoding of neural networks for GA's. An experiment is performed using a fitness function parameterized on network size and error rate. GA's with indirect encoding converge faster, but suboptimally. GA's with direct encoding converge slower, but with more incremental improvements, and get closer to optimality, both in network size (number of connections) and error rate.
   - The **direct encoding** of a neural network saves every individual connection (weight) as an entry in an adjacency matrix, and a vector of whether or not connections are 'active'. Direct encodings specify every connection, and take O(n^2) space.
   - An **indirect encoding** represents the structure of the network as a binary tree using a set of grammar rules. It can be represented as a string (serialization) with O(n) space. Indirect encoding uses rules for building a network - it "describes the pattern of connectivity compactly" ([HyperNEAT](http://eplex.cs.ucf.edu/hyperNEATpage/)).

 - ### Schmidhuber_Discovering_Problem_Solutions_with_Low_Kolmogorov_Complxity_and_High_Generalization_Capability
   - #### Overview
     - The paper describes a sample efficient turing-machine based random-search algorithm. The algorithm was used to tune the weights of a perceptron network on various simplistic tasks, including counting inputs and adding input positions (note: this is simple given the summation that occurs following the input-weight multiplication in the perceptron model). Performance was analyzed, and generalization properties of models with perfect training-data predictions were fantastic - well beyond the ability of backprop-based learners! An analysis was performed regarding the choice of primitives (operations) allowing the generation of solutions. Algorithmic information theory tells us that choosing primitives can only delay the optimal search by no more than a constant factor, as long as the primitives form a universal set - due to the certainty that primitives from another complete set being able to compose them. The authors acknowledge the shortcomings of random searches in particularly unsupervised, reward-delayed, or limited-feedback environments. They propose a mutation-only genetic-algorithm (GA) for integrating an incremental search algorithm, which was successful in improving learning speed.
     - The paper improves generalization by choosing to learn the algorithm that generates weights, rather than learning the weights themselves. This is very similar to indirect encoding in GA's, which funnily enough is what I was reading about when I stumbled across this paper.
   - #### Unexpected Design choices:
     - The solution candidate was only tested on the test data if it performed with 100% accuracy on the training data.
   - #### Results Commentary: Counting 1 Inputs
     - Each program generated was "trained" on only three training examples.
     - Only 20 solutions fulfilled the 100% accuracy on training data requirement to count as "successes". This classification was a prerequisite to running on testing data.
     - Out of these 20, only 3 training exemplars did not lead to perfect generalization on the 161,697 ($\binom{100}{3} - 3$) unseen test cases.
     - The optimal solution was not found, however the best solution seemed to be leq (less than or equal to) a factor of 3 worse in space performance, and leq a factor of 2 worse in time performance.
   - #### Results Commentary: Summing Positions of Inputs
     - 10 successes were found out of $5.5 * 10^7$ runs. Only 2 of the 10 led to imperfect generalization.
   - #### Definitions
     - The **Universal Prior**, also known as the **Solomonoff-Levin Distribution**, is presented as "the only method for assigning probabilities to hypotheses". It is defined as the probability of guessing a program p that computes bitstring s, given a program generated by random probability. Given the simplistic - but without loss of generality - alphabet of {0,1}, this works out to be:
       - $P_U(s) = \sum\limits_{p:f_u(p)=s}{(\frac{1}{2})^{|p|}}$
     - The **Universal or Levin Search** is a "computable, time-bounded version of Algorithmic \[Kolmogorov\] Complexity. Rather than running each program sequentially, and therefore being more likely to get stuck at a non-halting program, levin search "interleaving the execution of all possible programs on a universal Turing machine, sharing computation time equally among them, until one of the executed programs manages to solve the given inversion problem. If there exists a program p , of length $l(p)$ , that can solve the problem in time time $(p)$ , then Universal Search will solve the problem in a time $2^{l(p)+1time(p)}$ at most" ([scholarpedia](http://www.scholarpedia.org/article/Universal_search)).
   -  #### My Interests in Future Work
     - The author mentions that the evolutionary algorithm used in the incremental search section is a smarter mutation strategy than a more conventional hill-climbing strategy, due to the inability of the latter to find necessary non-trivial (spatially distant) correlated mutations in a practical time frame.
     - Solomonoff described methods for giving names to subprograms and using them as more complex primitives (Solomonoff 1964; 1686). This suffers from the same operation search-space explosion (denoted the "code explosion problem") due to the exponential search-space of programs with linear increases in allowed operations. Paul and Solomonoff (1991) address this issue with grouping of programs into "directories" of subprograms (?).
     - Solomonoff (1986) also proposed that the code explosion problem should be addressed by spending time on successful code compression. However, this was not pursued.
     - "I think i've found the trio of computational theory: Chomsky, Solomonoff, and Kolmogorov" (Carwyn).